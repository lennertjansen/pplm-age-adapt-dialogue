{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "steady-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import for n gram\n",
    "# Import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # for cool plotting\n",
    "import re # for regular expression\n",
    "import nltk # natural language processing toolkit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from langdetect import detect, detect_langs # for language detection\n",
    "# from tqdm.notebook import tqdm, trange\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "insured-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds for reproducibility\n",
    "SEED = 7\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedicated-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed datasets\n",
    "# Temporary fix for case analysis....\n",
    "train_df = pd.read_csv('../data/bnc/ca_splits/bnc_rb_ca_trainset_case_analysis.csv', encoding=\"utf-8\")\n",
    "val_df = pd.read_csv('../data/bnc/ca_splits/bnc_rb_ca_valset_case_analysis.csv', encoding=\"utf-8\")\n",
    "test_df = pd.read_csv('../data/bnc/ca_splits/bnc_rb_ca_testset_case_analysis.csv', encoding=\"utf-8\")\n",
    "\n",
    "# reset indices of subsets\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# rename column\n",
    "train_df.rename(columns={'label': 'age_cat','age_cat' : 'label'}, inplace=True)\n",
    "val_df.rename(columns={'label': 'age_cat','age_cat' : 'label'}, inplace=True)\n",
    "test_df.rename(columns={'label': 'age_cat','age_cat' : 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "manufactured-nursing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty instanes: 0\n",
      "Number of empty instanes: 0\n",
      "Number of empty instanes: 0\n"
     ]
    }
   ],
   "source": [
    "# Add preprocessing steps to produce column with clean_text including stopwords\n",
    "\n",
    "df_list = [train_df, val_df, test_df]\n",
    "\n",
    "for temp_df in df_list:\n",
    "\n",
    "    temp_df['clean_text_ws'] = temp_df['text'] # uncomment this and comment line above if you want incl. non-alph chars\n",
    "\n",
    "    # make all letters lowercase\n",
    "    temp_df['clean_text_ws'] = temp_df['clean_text_ws'].apply(lambda x: x.lower())\n",
    "\n",
    "    # remove whitespaces from beginning or ending\n",
    "    temp_df['clean_text_ws'] = temp_df['clean_text_ws'].apply(lambda x: x.strip())\n",
    "\n",
    "    print(f\"Number of empty instanes: {len(temp_df[temp_df.clean_text_ws == ''])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "technical-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "def print_evaluation_scores(labels, preds):\n",
    "    print(f\"Accuracy: {accuracy_score(labels, preds)}\")\n",
    "    print(f\"F1 score: {f1_score(labels, preds, average = None)}\") # outputs F1 per class\n",
    "    print(f\"Average precision: {average_precision_score(labels, preds, average = 'micro')}\")\n",
    "    print(f\"Average recall: {recall_score(labels, preds, average = 'micro')}\")\n",
    "    print(classification_report(labels, preds, digits=5, zero_division=0))\n",
    "    # print(f\"Confusion Matrix: {confusion_matrix(labels.argmax(axis=1), preds.argmax(axis=1))}\")\n",
    "    \n",
    "def print_top_n(vectorizer, clf, class_labels, n_feat = 10):\n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    for i, class_label in enumerate(class_labels):\n",
    "#         pdb.set_trace()\n",
    "        topn = np.argsort(clf.estimators_[i].coef_)[0][-n_feat:]\n",
    "        print(\"%s: %s\" % (class_label,\n",
    "              \" \".join(feature_names[j] for j in topn)))\n",
    "\n",
    "def most_informative_feature_for_class(vectorizer, classifier, class_labels, n=10):\n",
    "    #labelid = list(classifier.classes_).index(classlabel)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        topn = sorted(zip(classifier.estimators_[i].coef_[0], feature_names))[-n:]\n",
    "        \n",
    "        for coef, feat in topn:\n",
    "            print(class_label, feat, coef)\n",
    "            \n",
    "# class_labels_list = ['13-17', '23-27', '33-47']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "noticed-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list1 = [21,2,3,4,5]\n",
    "# np.argsort(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "desperate-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    '''For binary case'''\n",
    "    # column names and instantiation of subset output dataframe\n",
    "    keys = ['19-29 n-gram', '19-29 coef.', '50-plus n-gram', \"50-plus coef.\"]\n",
    "    df = pd.DataFrame(columns=keys)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        df.loc[0 if pd.isnull(df.index.max()) else df.index.max() + 1] = [fn_1] + [coef_1] + [fn_2] + [coef_2]\n",
    "        \n",
    "#     # Save dataframe to csv\n",
    "#     df.to_csv(\n",
    "#         f'mi_ngrams/bnc_trigram_{n}_mi_ngrams.csv',\n",
    "#         index=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "requested-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_ngram(train_df, test_df, n_grams=[3], seeds=[SEED], dataset='bnc_rb'):\n",
    "\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # results dict\n",
    "    accs_all = {}\n",
    "    if dataset == 'blog':\n",
    "        class_labels_list = ['13-17', '23-27', '33-47']\n",
    "    elif dataset == 'bnc' or dataset == 'bnc_rb':\n",
    "        class_labels_list = ['19_29', '50_plus']\n",
    "\n",
    "    test_accs = {}\n",
    "    test_f1s = {}\n",
    "\n",
    "    for n_gram in n_grams:\n",
    "        test_accs[n_gram] = {}\n",
    "        test_f1s[n_gram] = {}\n",
    "        # for class_label in class_labels_list:\n",
    "        #     test_f1s[n_gram][class_label] = {}\n",
    "\n",
    "    print(\"Starting training and testing loops...\")\n",
    "    for seed in tqdm(seeds, desc = \"Seed loop.\"):\n",
    "\n",
    "        for n in tqdm(n_grams, desc = \"n gram loop.\"):\n",
    "\n",
    "            # Split data into features/ X and labels / Y\n",
    "            # X = data['clean_data']\n",
    "            # Y = data['labels']\n",
    "\n",
    "            # n-gram model\n",
    "            vectorizer = CountVectorizer(binary = True, ngram_range = (1, n))\n",
    "\n",
    "            # fit vectorization model\n",
    "            concat_df = pd.concat([train_df, test_df])\n",
    "            X = vectorizer.fit_transform(concat_df['clean_text_ws']) # remove \"_ws\" to do analysis without stopwords\n",
    "\n",
    "            X_train = X[:-len(test_df)]\n",
    "            X_test = X[-len(test_df):]\n",
    "\n",
    "            # Binarize the labels for prediction\n",
    "            if dataset == 'blog':\n",
    "                # binarizer = MultiLabelBinarizer(classes = sorted(label_counts.keys()))\n",
    "                binarizer = LabelBinarizer()\n",
    "            elif dataset == 'bnc_rb' or dataset == 'bnc':\n",
    "                binarizer = LabelBinarizer()\n",
    "\n",
    "            # Y = binarizer.fit_transform(train_df.age_cat)\n",
    "            Y_train = train_df.label\n",
    "            Y_test = test_df.label\n",
    "\n",
    "            # label_counts.keys()\n",
    "\n",
    "            # Split data into train and test sets\n",
    "            # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_size)\n",
    "\n",
    "            # Fit logistic regression model\n",
    "            start_time = time.time()\n",
    "            model = LogisticRegression(solver = 'lbfgs', multi_class='ovr', max_iter = 1000000)\n",
    "            model = OneVsRestClassifier(model)\n",
    "            # model = MultiOutputClassifier(model)\n",
    "            model.fit(X_train, Y_train)\n",
    "            print(f\"Fitting model took {time.time() - start_time} seconds.\")\n",
    "\n",
    "            # make predictions on test set\n",
    "            Y_pred = model.predict(X_test)\n",
    "\n",
    "            # Y_pred_inversed = binarizer.inverse_transform(Y_pred)\n",
    "            # Y_test_inversed = binarizer.inverse_transform(Y_test)\n",
    "\n",
    "            print(\"=\" * 81)\n",
    "\n",
    "            print(f\"n = {n}\")\n",
    "            print(f\"seed = {seed}\")\n",
    "            print_evaluation_scores(Y_test, Y_pred)\n",
    "\n",
    "            test_accs[n][seed] = accuracy_score(Y_test, Y_pred)\n",
    "            test_f1s[n][seed] = f1_score(Y_test, Y_pred, average=None)\n",
    "\n",
    "            # for label_idx in range(len(class_labels_list)):\n",
    "            #     test_f1s[n][class_labels_list[label_idx]][seed] = f1_score(Y_test, Y_pred, average=None)[label_idx]\n",
    "\n",
    "            if n in accs_all:\n",
    "                accs_all[n].append(accuracy_score(Y_test, Y_pred))\n",
    "            else:\n",
    "                accs_all[n] = [accuracy_score(Y_test, Y_pred)]\n",
    "\n",
    "            # Print most informative features\n",
    "            # if n == 1:\n",
    "            #     print(\"Most informative features per age-group.\")\n",
    "            #     print_top_n_thresh(vectorizer = vectorizer, clf = model,\n",
    "            #                 class_labels = class_labels_list, n_feat = 20)\n",
    "            \n",
    "            # Print most informative features\n",
    "            print(\"Most informative features per age-group.\")\n",
    "#             print_top_n(vectorizer = vectorizer, clf = model, class_labels = class_labels_list, n_feat = 5)\n",
    "            show_most_informative_features(vectorizer=vectorizer, clf=model, n=100)\n",
    "\n",
    "            print(\"-\" * 81)\n",
    "    #         print(\"Some failure cases.\")\n",
    "    # #         predictions = model.predict(inputs)\n",
    "    #         for i, (x, pred, label) in enumerate(zip(X_test, Y_pred, Y_test)):\n",
    "    #             if (pred != label).any():\n",
    "    #                 print(f\"pred: {pred}\")\n",
    "    #                 print(f\"label: {label}\")\n",
    "    #                 pred_cat = binarizer.classes_[np.where(pred == 1)[0][0]]\n",
    "    #                 label_cat = binarizer.classes_[np.where(label == 1)[0][0]]\n",
    "    #                 print(data['clean_data'][i], 'has been classified as ', pred_cat, 'and should be ', label_cat)\n",
    "\n",
    "            print(\"=\" * 81)\n",
    "\n",
    "\n",
    "            int_labels = [label for label in range(len(class_labels_list))]\n",
    "            cm = confusion_matrix(Y_test, Y_pred, labels=int_labels)\n",
    "            # make_confusion_matrix(cf=cm, categories=class_labels_list, title=f'Confusion Matrix for {dataset} on Test set',\n",
    "            #                       num_labels=int_labels, y_true=Y_test, y_pred=Y_pred, figsize=FIGSIZE)\n",
    "            cur_datetime = datetime.now().strftime('%d_%b_%Y_%H_%M_%S')\n",
    "            # plt.savefig(f\"{FIGDIR}{dataset}/cm_{n}_gram_{dataset}_dt_{cur_datetime}.png\",\n",
    "            #             bbox_inches='tight')\n",
    "\n",
    "    #         most_informative_feature_for_class(vectorizer = vectorizer, classifier = model, class_labels = class_labels_list, n=10)\n",
    "\n",
    "#             df = pd.read_csv('bnc_rb_10p_testset_case_analysis.csv')\n",
    "\n",
    "#             df.insert(len(df.columns), 'trigram_pred', Y_pred)\n",
    "\n",
    "#             # Save dataframe to csv\n",
    "#             df.to_csv(\n",
    "#                 'bnc_rb_10p_testset_case_analysis.csv',\n",
    "#                 index=False\n",
    "#             )\n",
    "\n",
    "\n",
    "    # print average metrics\n",
    "    print(89*'-')\n",
    "    print(89 * '-')\n",
    "    print(\"PRINTING AVERAGE METRICS\")\n",
    "    for n_gram in n_grams:\n",
    "        n_gram_accs = []\n",
    "        n_gram_f1s = []\n",
    "        for seed in seeds:\n",
    "            n_gram_accs.append(test_accs[n_gram][seed])\n",
    "            n_gram_f1s.append(test_f1s[n_gram][seed])\n",
    "\n",
    "        print(f\"| n = {n_gram} | Average accuracy = {np.mean(n_gram_accs)} | Acc std = {np.std(n_gram_accs)} \"\n",
    "              f\"| Average f1s = {np.mean(n_gram_f1s, axis=0)} | F1s std = {np.std(n_gram_f1s, axis=0)} |\")\n",
    "\n",
    "    overall_end_time = time.time()\n",
    "\n",
    "\n",
    "    print(f\"Done with everything. Took {overall_end_time - overall_start_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "logical-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed loop.:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "n gram loop.:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training and testing loops...\n",
      "Fitting model took 31.423712968826294 seconds.\n",
      "=================================================================================\n",
      "n = 3\n",
      "seed = 7\n",
      "Accuracy: 0.7167923657072495\n",
      "F1 score: [0.72380666 0.70941251]\n",
      "Average precision: 0.6570194087667262\n",
      "Average recall: 0.7167923657072495\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70745   0.74093   0.72381      3254\n",
      "           1    0.72710   0.69257   0.70941      3243\n",
      "\n",
      "    accuracy                        0.71679      6497\n",
      "   macro avg    0.71727   0.71675   0.71661      6497\n",
      "weighted avg    0.71726   0.71679   0.71662      6497\n",
      "\n",
      "Most informative features per age-group.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lennertjansen/miniconda3/envs/thesis/lib/python3.7/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). If you observe this warning while using RFE or SelectFromModel, use the importance_getter parameter instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-3.2079\tum             \t\t2.3738\tyes            \n",
      "\t-2.8439\tcool           \t\t2.1240\tyou know       \n",
      "\t-2.5770\tshit           \t\t2.0938\twonderful      \n",
      "\t-2.1180\thmm            \t\t1.9017\thow weird      \n",
      "\t-2.0908\tlike           \t\t1.8413\tchinese        \n",
      "\t-2.0221\twas like       \t\t1.7360\tright          \n",
      "\t-1.9588\tlove           \t\t1.7095\tbuilding       \n",
      "\t-1.9582\tas well        \t\t1.6576\tright right    \n",
      "\t-1.8822\tas in          \t\t1.5475\tso erm         \n",
      "\t-1.8353\tcute           \t\t1.4347\tmm mm          \n",
      "\t-1.8207\tuni            \t\t1.4079\tcheers         \n",
      "\t-1.7928\tmassive        \t\t1.3854\tshed           \n",
      "\t-1.7898\twanna          \t\t1.3696\tpain           \n",
      "\t-1.7815\tfuck           \t\t1.3601\twe know        \n",
      "\t-1.7219\ttut            \t\t1.3511\tlaugh yeah exactly\n",
      "\t-1.6155\tfucking        \t\t1.3471\tordinary       \n",
      "\t-1.5800\tbut yeah       \t\t1.3194\tmother         \n",
      "\t-1.5650\tmhm            \t\t1.3187\toperation      \n",
      "\t-1.5577\tbasically      \t\t1.3088\tgarden         \n",
      "\t-1.5320\tjesus          \t\t1.2988\tll be nice     \n",
      "\t-1.5269\tgrand          \t\t1.2819\tgosh           \n",
      "\t-1.4768\tbitch          \t\t1.2638\twell           \n",
      "\t-1.4650\tlol            \t\t1.2526\taha            \n",
      "\t-1.4519\tpretty         \t\t1.2518\tschool         \n",
      "\t-1.4353\tridiculous     \t\t1.2452\temail          \n",
      "\t-1.4286\tawesome        \t\t1.2341\teverybody      \n",
      "\t-1.4279\tyay            \t\t1.2281\tyellow         \n",
      "\t-1.4220\tyeah definitely\t\t1.2274\tin in          \n",
      "\t-1.4038\twas amazing    \t\t1.2258\tlord           \n",
      "\t-1.3964\tooh            \t\t1.2145\tto look        \n",
      "\t-1.3954\tweird          \t\t1.1897\tphotographs    \n",
      "\t-1.3924\tand yeah       \t\t1.1875\tglasses        \n",
      "\t-1.3824\tguess          \t\t1.1851\tgood good      \n",
      "\t-1.3510\tpizza          \t\t1.1701\ttrombone       \n",
      "\t-1.3496\tannoying       \t\t1.1641\tfine thanks    \n",
      "\t-1.3181\tmate           \t\t1.1602\tthat right     \n",
      "\t-1.3007\tright now      \t\t1.1416\tgot no idea    \n",
      "\t-1.2911\tunclear unclear\t\t1.1407\tyes yes        \n",
      "\t-1.2882\texcited        \t\t1.1235\tabsolutely     \n",
      "\t-1.2867\thate           \t\t1.1196\tof course      \n",
      "\t-1.2773\tshut           \t\t1.1181\tborn           \n",
      "\t-1.2771\tbad            \t\t1.1180\tapple          \n",
      "\t-1.2689\tfood           \t\t1.1161\tis is          \n",
      "\t-1.2612\tsaid yes       \t\t1.1127\tand and        \n",
      "\t-1.2487\tknow what mean \t\t1.1067\tgoodness me    \n",
      "\t-1.2398\tyeah of course \t\t1.1054\tgood fun       \n",
      "\t-1.2359\tmiss           \t\t1.1020\thospice        \n",
      "\t-1.2354\tand stuff      \t\t1.0956\tyep yeah       \n",
      "\t-1.2215\tyeah of        \t\t1.0833\tno exactly     \n",
      "\t-1.2205\tjust don       \t\t1.0815\tlawn           \n",
      "\t-1.2121\tscared         \t\t1.0770\tbusiness       \n",
      "\t-1.2017\tand then       \t\t1.0731\tlike your      \n",
      "\t-1.1957\tdefinitely     \t\t1.0730\tpoodle         \n",
      "\t-1.1831\tyou don have   \t\t1.0721\tit it          \n",
      "\t-1.1651\tto like        \t\t1.0676\tdon know unclear\n",
      "\t-1.1636\tyes please     \t\t1.0674\tnot just       \n",
      "\t-1.1600\tdo you know    \t\t1.0641\tdarling        \n",
      "\t-1.1558\tabout you      \t\t1.0621\tor or          \n",
      "\t-1.1558\too             \t\t1.0617\tlaugh yeah pause\n",
      "\t-1.1512\tmean yeah      \t\t1.0585\tokay fine      \n",
      "\t-1.1506\tcoke           \t\t1.0565\tnot nice       \n",
      "\t-1.1437\tmental         \t\t1.0513\tmind you       \n",
      "\t-1.1349\tshort you know \t\t1.0509\tno sure        \n",
      "\t-1.1215\tfun            \t\t1.0460\tyou come       \n",
      "\t-1.1137\tgym            \t\t1.0402\tand so         \n",
      "\t-1.1099\tyes indeed     \t\t1.0318\tmisty          \n",
      "\t-1.1094\tnice           \t\t1.0305\tchurch         \n",
      "\t-1.1071\toh okay        \t\t1.0303\ttank           \n",
      "\t-1.1055\tit fine        \t\t1.0303\tcheers mate    \n",
      "\t-1.1055\tso good        \t\t1.0269\tthe the        \n",
      "\t-1.1050\tfriends        \t\t1.0231\txylophone      \n",
      "\t-1.1017\tno way         \t\t1.0218\thuge           \n",
      "\t-1.1012\tcat            \t\t1.0208\tchap           \n",
      "\t-1.0902\tnope           \t\t1.0184\tcharity        \n",
      "\t-1.0778\tfourteen       \t\t1.0174\tlike this      \n",
      "\t-1.0770\thilarious      \t\t1.0157\toh don know    \n",
      "\t-1.0714\twas really     \t\t1.0134\tso what are    \n",
      "\t-1.0660\treally good    \t\t1.0130\ttrunc trunc it \n",
      "\t-1.0635\tbaby           \t\t1.0101\tright oh       \n",
      "\t-1.0630\tgonna          \t\t1.0096\tpaint          \n",
      "\t-1.0587\tbless          \t\t1.0081\tmy word        \n",
      "\t-1.0576\tugly           \t\t1.0081\toh my word     \n",
      "\t-1.0570\tmy gosh        \t\t1.0069\thorses         \n",
      "\t-1.0562\toh my gosh     \t\t1.0054\tbin            \n",
      "\t-1.0511\tso well        \t\t1.0050\tdaughter       \n",
      "\t-1.0504\twee            \t\t1.0046\term and        \n",
      "\t-1.0420\tsmaller        \t\t1.0007\tbath           \n",
      "\t-1.0375\tdate           \t\t0.9957\tsensible       \n",
      "\t-1.0348\tnight          \t\t0.9910\tdidn she       \n",
      "\t-1.0323\tdesc misc yes  \t\t0.9876\tcomputer       \n",
      "\t-1.0323\tmisc yes       \t\t0.9820\textraordinary  \n",
      "\t-1.0308\tguy            \t\t0.9803\tmuch more      \n",
      "\t-1.0217\tphoto          \t\t0.9772\tnot anon nametype\n",
      "\t-1.0215\tegg            \t\t0.9744\toccasional     \n",
      "\t-1.0198\tgarlic         \t\t0.9728\thow amazing    \n",
      "\t-1.0196\tfor like       \t\t0.9717\tin china       \n",
      "\t-1.0175\tyeah that true \t\t0.9711\tbe back        \n",
      "\t-1.0173\tliterally      \t\t0.9609\tsandwiches     \n",
      "\t-1.0145\thonestly       \t\t0.9607\tpension        \n",
      "\t-1.0143\tlike yeah      \t\t0.9579\tcrikey         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "n gram loop.: 100%|██████████| 1/1 [00:39<00:00, 39.43s/it]\u001b[A\n",
      "Seed loop.: 100%|██████████| 1/1 [00:39<00:00, 39.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "=================================================================================\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "PRINTING AVERAGE METRICS\n",
      "| n = 3 | Average accuracy = 0.7167923657072495 | Acc std = 0.0 | Average f1s = [0.72380666 0.70941251] | F1s std = [0. 0.] |\n",
      "Done with everything. Took 39.438562870025635 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "concat_train_df = pd.concat([train_df, val_df])\n",
    "train_test_ngram(train_df=concat_train_df, test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_top_n(vectorizer, clf, class_labels, n_feat = 10):\n",
    "#     \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "#     feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "#     for i, class_label in enumerate(class_labels):\n",
    "#         pdb.set_trace()\n",
    "#         topn = np.argsort(clf.estimators_[i].coef_)[0][-n_feat:]\n",
    "#         print(\"%s: %s\" % (class_label,\n",
    "#               \" \".join(feature_names[j] for j in topn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.clean_text_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_dict = set(stopwords.words('english')) # use set (hash table) data structure for faster lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-colony",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
